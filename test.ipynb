{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "221848f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas\n",
    "import time\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import text_processing as tp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0909c6e1",
   "metadata": {},
   "source": [
    "Pre-defined params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fac65057",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_proportion = 0.15\n",
    "eval_proportion = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e45f353",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEN_LEN = 262 # хард лок длины, воизбежание ошибки\n",
    "MAX_TOKEN_GENERATED = 70\n",
    "TOKENS_TRESHOLD_FREQ = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb1178c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "EPOCHS = 10\n",
    "LR = 0.001\n",
    "\n",
    "RNN_HIDDEN_SIZE = 324\n",
    "FC_HIDDEN_SIZE = 512\n",
    "NUM_CHANNELS_CNN = 256\n",
    "EMBEDDING_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "895d5724",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE_FILEPATH = 'RNN_params.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f14ffb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4206c022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9ddeb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalyzerMLP(nn.Module):\n",
    "    def __init__(self, input_cnt, output_cnt, hidden_size):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(input_cnt, hidden_size),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_size, output_cnt)\n",
    "        )\n",
    "    def forward(self, x_data, apply_softmax=False):\n",
    "        y_out = self.linear_stack(x_data)\n",
    "        if apply_softmax:\n",
    "            y_out = nn.functional.softmax(y_out, dim=1)\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73ae5acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalyzerCNN(nn.Module):\n",
    "    def __init__(self, in_channels, num_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.convnet = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, num_channels, kernel_size=3),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(num_channels, num_channels, kernel_size=3, stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(num_channels, num_channels, kernel_size=3, stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(num_channels, num_channels, kernel_size=3),\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "        self.fc = nn.Linear(num_channels, num_classes)\n",
    "\n",
    "    def forward(self, x_data, apply_softmax=False):\n",
    "        fratures = self.convnet(x_data).squeeze(dim=2)\n",
    "        y_out = self.fc(fratures)\n",
    "        if apply_softmax:\n",
    "            y_out = torch.nn.functional.softmax(y_out, dim=1)\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8e7946b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalyzerEmbedCNN(nn.Module):\n",
    "    def __init__(self, embedding_size, num_embeddings,\\\n",
    "                 num_channels, hidden_size, num_classes, kernel_size=3, pretrained_embeddings = None, padding_idx=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        if pretrained_embeddings is None:\n",
    "            self.embed = nn.Embedding(num_embeddings, embedding_size, padding_idx)\n",
    "        else:\n",
    "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
    "            self.embed = nn.Embedding(num_embeddings, embedding_size, padding_idx, _weight=pretrained_embeddings)\n",
    "\n",
    "        self.convnet = nn.Sequential(\n",
    "            nn.Conv1d(embedding_size, num_channels, kernel_size=kernel_size),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(num_channels, num_channels, kernel_size=kernel_size, stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(num_channels, num_channels, kernel_size=kernel_size, stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(num_channels, num_channels, kernel_size=kernel_size),\n",
    "            nn.ELU(),\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(num_channels, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_data, apply_softmax=False):\n",
    "        # x_data - vector of indices\n",
    "        embed_vectors = self.embed(x_data).permute(0, 2, 1) # permutation to make embedding_dimensionality input_channels in CNN.\n",
    "                                                            # now each token embedding vector are collumn, not a row\n",
    "        \n",
    "        features = self.convnet(embed_vectors).squeeze(dim=2)\n",
    "        y_out = self.fc(features)\n",
    "        if apply_softmax:\n",
    "            y_out = nn.functional.softmax(y_out, dim=1)\n",
    "\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1408c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalyzerEmbedRNN(nn.Module):\n",
    "    def __init__(self, embed_size, num_embed, rnn_hidden_size, fc_hidden_size, num_classes, padding_idx=0, batch_first=True, pretrained_embeddings=None):\n",
    "        super().__init__()\n",
    "\n",
    "        if pretrained_embeddings is None:\n",
    "            self.embed = nn.Embedding(num_embed, embed_size, padding_idx)\n",
    "        else:\n",
    "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
    "            self.embed = nn.Embedding(num_embed, embed_size, padding_idx, _weight=pretrained_embeddings)\n",
    "        \n",
    "        self.rnn = nn.GRU(embed_size, rnn_hidden_size, batch_first=batch_first)\n",
    "\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(rnn_hidden_size, fc_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc_hidden_size, num_classes),\n",
    "        )\n",
    "    def forward(self, x_data, useful_len, use_packing=True, apply_softmax=False):\n",
    "        # print('raw_size = ', x_data.size())\n",
    "        # time.sleep(5)\n",
    "\n",
    "        embedded = self.embed(x_data)\n",
    "        # print('after embed size = ', embedded.size())\n",
    "        # time.sleep(5)\n",
    "\n",
    "        # Упаковка для эффективности. Паддинг не будет участвовать в расчетах. Полезная часть каждого предложения определяется переменной useful_length\n",
    "        # Как показали опыты, лучше всегда упаковывать тензоры для RNN(Результаты лучше на ~15%)\n",
    "        if use_packing:\n",
    "            packed = nn.utils.rnn.pack_padded_sequence(embedded, useful_len.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        else:\n",
    "            packed = embedded\n",
    "        #print('packed size = ', packed.size())\n",
    "        #time.sleep(5)\n",
    "\n",
    "        outputs, hiddens = self.rnn(packed) # hiddens = [Direction∗num_layers, batch_size, hidden_size] - финальное скрытое состояноя для каждого слоя RNN\n",
    "        # print('hidden size = ', hiddens.size())\n",
    "        # time.sleep(5)\n",
    "\n",
    "        hiddens = nn.functional.dropout(hiddens, 0.5)\n",
    "\n",
    "        y_out = self.linear_stack(hiddens[-1])\n",
    "        # print('after linear size = ',y_out.size())\n",
    "        # time.sleep(5)\n",
    "\n",
    "        if apply_softmax:\n",
    "            y_out = nn.functional.softmax(y_out, dim=1)\n",
    "        \n",
    "        return y_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8118f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenGenerationRNN(nn.Module):\n",
    "    def __init__(self, num_tokens_embed, num_label_embed, embed_size, rnn_hidden_size,\\\n",
    "                 fc_hidden_size, num_classes, padding_idx=0, batch_first=True, pretrained_embeddings=None):\n",
    "        super().__init__()\n",
    "\n",
    "        if pretrained_embeddings is None:\n",
    "            self.token_embed = nn.Embedding(num_tokens_embed, embed_size, padding_idx)\n",
    "        else:\n",
    "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
    "            self.token_embed = nn.Embedding(num_tokens_embed, embed_size, padding_idx, _weight=pretrained_embeddings)\n",
    "        \n",
    "        self.label_embed = nn.Embedding(num_label_embed, embed_size, padding_idx)\n",
    "\n",
    "        self.rnn = nn.GRU(embed_size, rnn_hidden_size, batch_first=batch_first)\n",
    "\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(rnn_hidden_size, fc_hidden_size),\n",
    "            nn.Dropout(),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(fc_hidden_size, num_tokens_embed),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x_data, useful_len, use_packing=True, apply_softmax=False):\n",
    "        # print('x_data ', x_data.size())\n",
    "\n",
    "        x_embed = self.token_embed(x_data)\n",
    "        # print('x_embed ', x_embed.size())\n",
    "\n",
    "        if use_packing:\n",
    "            # Упаковываем последовательности\n",
    "            packed = nn.utils.rnn.pack_padded_sequence(x_embed, useful_len.cpu(), batch_first=True, enforce_sorted=False)\n",
    "\n",
    "            packed_outputs, last_hidden_vec = self.rnn(packed)\n",
    "\n",
    "            # Распаковываем обратно в тензор с паддингом\n",
    "            outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first=True, padding_value=0.0)\n",
    "        else:\n",
    "            outputs, last_hidden_vec = self.rnn(x_embed)\n",
    "\n",
    "        # print('outputs before reshape ', outputs.size())\n",
    "\n",
    "        batch_size, seq_size, feature_size = outputs.shape\n",
    "        y_out = outputs.reshape(batch_size * seq_size, feature_size) # Матрица, где каждый элемент представляет отдельное предсказание(такая форма необходима для полносвязного слоя)\n",
    "\n",
    "        # print('outputs after reshape ', y_out.size())\n",
    "\n",
    "        y_out = self.linear_stack(y_out)\n",
    "\n",
    "        # print('y_out before reshape ', y_out.size())\n",
    "\n",
    "        if apply_softmax:\n",
    "            y_out = nn.functional.softmax(y_out, dim=1)\n",
    "        \n",
    "        new_features_size = y_out.size(1)\n",
    "        y_out = y_out.reshape(batch_size, seq_size, new_features_size)\n",
    "\n",
    "        # print('y_out after reshape ', y_out.size())\n",
    "\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e6f18c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle=True, drop_last=True, device='cpu'):\n",
    "    dataloader = DataLoader(dataset, batch_size, shuffle, drop_last=drop_last)\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for key, tensor in data_dict.items():\n",
    "            out_data_dict[key] = data_dict[key].to(device) # Sending tensors to propper device\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12c9309a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def samples_from_model(model, vectorizer : tp.Vectorizer, seq_size, seq_count=1, temperature=1.0):\n",
    "    '''Returns tensor of generated indices [seq_count, seq_size].\\\n",
    "        vecatorizer: token vectorizer\\\n",
    "        seq_size: max len of the generated seq\\\n",
    "        seq_count: count of sequences to generate\\\n",
    "        '''\n",
    "    model.eval()\n",
    "    begin_seq_index = [vectorizer.tokens_vocab._bos_index for _ in range(seq_count)]\n",
    "    begin_seq_index = torch.tensor(begin_seq_index, dtype=torch.int64).unsqueeze(dim=1) # [seq_count, 1]\n",
    "    indices = [begin_seq_index] # [seq_count, 1]\n",
    "\n",
    "    for timestamp in range(seq_size):\n",
    "        x_t = indices[timestamp] # get the last element\n",
    "        x_t_embed = model.token_embed(x_t) # [seq_count, 1, embed_size]\n",
    "        rnn_out, hidden_last = model.rnn(x_t_embed) # [seq_count, 1, rnn_hidden_size]\n",
    "        rnn_out = rnn_out.squeeze(dim=1) # [seq_count, rnn_hidden_size]\n",
    "        prediction = model.linear_stack(rnn_out) # [seq_count, vocab_size]\n",
    "        proba_vec = nn.functional.softmax(prediction/temperature, dim=1)\n",
    "        indices.append(torch.multinomial(proba_vec, num_samples=1)) # [seq_count, 1] # selecting one el from multinomial distribution\n",
    "    indices = torch.stack(indices).squeeze() # [seq_size, seq_count]\n",
    "    return indices.permute(1, 0) # [seq_count, seq_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2ceed30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_indices(indices, vectorizer) -> list[str]:\n",
    "    seq_count, seq_len = (indices.size(0), indices.size(1))\n",
    "    vocab = vectorizer.tokens_vocab\n",
    "    decoded = []\n",
    "    for seq in range(seq_count):\n",
    "        string = ''\n",
    "        for idx in range(seq_len):\n",
    "            index = indices[seq, idx].item()\n",
    "            if index != vocab.mask_token_index:\n",
    "                string += vocab.get_token(index) + ' '\n",
    "            if index == vocab._eos_index:\n",
    "                break\n",
    "        decoded.append(string)\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a369cfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sizes(prediction, target):\n",
    "    '''Normalize tensor sizes for loss computing'''\n",
    "    if len(prediction.size()) == 3:\n",
    "        prediction = prediction.reshape(-1, prediction.size(2))\n",
    "    if len(target.size()) == 2:\n",
    "        target = target.reshape(-1)\n",
    "    # print('normalize_sizes predicton.size() ', prediction.size())\n",
    "    # print('normalize_sizes target.size() ', target.size())\n",
    "    return prediction, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82841456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_loss(prediction, target, mask_index=0):\n",
    "    '''mask index: index to be ignored in loss computation'''\n",
    "    prediction, target = normalize_sizes(prediction, target)  # returns prediction matrix [batch_size*seq_len, vocab_size], \n",
    "                                                            # target: 1d tensor of correct indices\n",
    "    # print('sequence_loss predicton.size() ', prediction.size())\n",
    "    # print('sequence_loss target.size() ', target.size())\n",
    "    return nn.functional.cross_entropy(prediction, target, ignore_index=mask_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6386780e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_seq(prediction, target, mask_index=0):\n",
    "    prediction, target = normalize_sizes(prediction, target)\n",
    "    _, pred_indices = torch.max(prediction, dim=1)\n",
    "    correct_indices = torch.eq(pred_indices, target).float()\n",
    "    all_valid_indices = torch.ne(target, mask_index).float()\n",
    "\n",
    "    num_correct = (correct_indices*all_valid_indices).sum().item()\n",
    "    num_valid = all_valid_indices.sum().item()\n",
    "\n",
    "    return (num_correct / num_valid) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "822c1bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_class_pred(prediction, target):\n",
    "    prediction_lables = prediction.max(dim=1)[1]\n",
    "    target_labels = target.max(dim=1)[1]\n",
    "    n_correct = torch.eq(prediction_lables, target_labels).sum().item()\n",
    "    return (100*n_correct)/len(prediction_lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5bf53ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_freq(dataframe : pandas.DataFrame, tokenizer, apply_lower=True):\n",
    "    tokens_freq = {}\n",
    "    labels_freq = {}\n",
    "    for i in range(len(dataframe)):\n",
    "        labels_freq[dataframe.loc[i, 'y_target']] = True # За один проход токенизируем текст и метки\n",
    "        tokens = tokenizer.tokenize(dataframe.loc[i, 'x_data'])\n",
    "        if apply_lower:\n",
    "            tokens = map(lambda x: x.lower(), tokens)\n",
    "        for token in tokens:\n",
    "            if token in tokens_freq:\n",
    "                tokens_freq[token] += 1\n",
    "            else:\n",
    "                tokens_freq[token] = 1\n",
    "    return tokens_freq, labels_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ee1216d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_to_file(model, filepath):\n",
    "    torch.save(model, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47313dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('D:/Files/Datasets/twitter_financial_news_sentiment/sent_train.csv')\n",
    "df = pd.read_csv('D:/Files/Datasets/ru_twitter_posts/negative.csv')\n",
    "# Максимум в отзывах 31 слово, разделенных пробелом. Max 190 символов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1346164f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'ttext' : 'x_data'})['x_data'].to_frame()\n",
    "df['y_target'] = 1\n",
    "# df = df.rename(columns={'text' : 'x_data', 'label' : 'y_target'})\n",
    "df['y_target'] = df['y_target'].apply(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a989b0c",
   "metadata": {},
   "source": [
    "setting train, test, evaluation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c899d43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['split'] = 'train'\n",
    "df_len = len(df)\n",
    "test_eval_idx = np.random.choice(df_len, int(df_len*(test_proportion + eval_proportion)), replace=False)\n",
    "\n",
    "test_eval_prop = test_proportion / eval_proportion\n",
    "val_len = int(len(test_eval_idx)/(test_eval_prop+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f6b180d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(test_eval_idx.size):\n",
    "    if i < val_len:\n",
    "        df.loc[test_eval_idx[i], 'split'] = 'validation'\n",
    "    else:\n",
    "        df.loc[test_eval_idx[i], 'split'] = 'test'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de45f6c",
   "metadata": {},
   "source": [
    "Замена адреса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e2b71dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['x_data'] = df['x_data'].apply(lambda x: re.sub(r'https?://.*', r'SOMEURL', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9279cd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tp.SeparatorTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90058122",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "lengthes = []\n",
    "for i in range(len(df)):\n",
    "    texts.append(tokenizer.tokenize(df.loc[i, 'x_data']))\n",
    "    lengthes.append(len(texts[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88a7f34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259\n"
     ]
    }
   ],
   "source": [
    "print(max(*lengthes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d282eaf",
   "metadata": {},
   "source": [
    "Первое заполнение словаря и сохранение в файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "587e3e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_vocabulary = tp.Vocabulary()\n",
    "label_vocabulary = tp.Vocabulary(is_lexical_tokens=False)\n",
    "\n",
    "tokens_freq, labels_freq = get_tokens_freq(df, tokenizer)\n",
    "\n",
    "for key, value in tokens_freq.items():\n",
    "    if value > TOKENS_TRESHOLD_FREQ:\n",
    "        tokens_vocabulary.add_token(key)\n",
    "\n",
    "for key, value in labels_freq.items():\n",
    "    label_vocabulary.add_token(key)\n",
    "\n",
    "\n",
    "tokens_vocabulary.to_json('tokens_vocab.json')\n",
    "label_vocabulary.to_json('label_vocab.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3f64f07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_vocabulary = tp.Vocabulary().from_json('tokens_vocab.json')\n",
    "label_vocabulary = tp.Vocabulary().from_json('label_vocab.json')\n",
    "vectorizer = tp.Vectorizer(tokens_vocabulary, label_vocabulary, MAX_SEN_LEN) # Необходимо знать max_sentence_len для использования сверточной НН"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b72054d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4327"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_vocabulary.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f316d9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tp.CustomDataset(df, tokenizer, vectorizer)\n",
    "batch_generator = generate_batches(dataset, BATCH_SIZE, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fbb98b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_index = tokens_vocabulary.mask_token_index\n",
    "# model = SentimentAnalyzerMLP(len(tokens_vocabulary), len(label_vocabulary), hidden_size)\n",
    "# model = SentimentAnalyzerCNN(tokens_vocabulary.size(), const_num_channels, label_vocabulary.size())\n",
    "# model = SentimentAnalyzerEmbedCNN(embedding_size=embedding_size, num_embeddings=len(tokens_vocabulary._token_to_idx),\\\n",
    "                                  # num_channels=num_channels, hidden_size=hidden_size, num_classes=3, kernel_size=3)\n",
    "# model = SentimentAnalyzerEmbedRNN(embed_size=embedding_size, num_embed=len(tokens_vocabulary._token_to_idx), rnn_hidden_size=rnn_hidden_size,\\\n",
    "                                  # fc_hidden_size=hidden_size, num_classes=3)\n",
    "model = TokenGenerationRNN(len(tokens_vocabulary._token_to_idx), len(label_vocabulary._token_to_idx), embed_size=EMBEDDING_SIZE,\\\n",
    "                           rnn_hidden_size=RNN_HIDDEN_SIZE, fc_hidden_size=FC_HIDDEN_SIZE, num_classes=3, padding_idx=mask_index)\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2a656208",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(MODEL_SAVE_FILEPATH, weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec1db64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "for epoch in range(EPOCHS):\n",
    "    print('epoch: ', epoch+1)\n",
    "    epoch_err = train_running_acc = validation_acc = 0\n",
    "    dataset.set_dataframe_split('train')\n",
    "    batch_generator = generate_batches(dataset, BATCH_SIZE, device=device)\n",
    "    model.train()\n",
    "    \n",
    "    for idx, batch in enumerate(batch_generator):\n",
    "        optimizer.zero_grad()\n",
    "        # print('batch[x_data].size() ', batch['x_data'].size())\n",
    "        # print('batch[useful_len].size() ', batch['useful_len'].size())\n",
    "\n",
    "        prediction = model(x_data=batch['x_data'], useful_len=batch['useful_len'], use_packing=False)\n",
    "        # print('prediction.size() ', prediction.size())\n",
    "\n",
    "        loss = sequence_loss(prediction, batch['y_target'], mask_index)\n",
    "\n",
    "        epoch_err += loss.item()\n",
    "        train_running_acc += (compute_accuracy_seq(prediction, batch['y_target'], mask_index)-train_running_acc)/(idx+1)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "    \n",
    "    print('train epoch_err: ', epoch_err)\n",
    "    # evaluating model perfomance each epoch\n",
    "    model.eval()\n",
    "    dataset.set_dataframe_split('validation')\n",
    "    batch_generator = generate_batches(dataset, BATCH_SIZE, device=device)\n",
    "    for idx, batch in enumerate(batch_generator):\n",
    "        prediction = model(batch['x_data'], batch['useful_len'], use_packing=False)\n",
    "        validation_acc += (compute_accuracy_seq(prediction, batch['y_target'], mask_index)-validation_acc)/(idx+1)\n",
    "\n",
    "    print('train accuracy: ', train_running_acc)\n",
    "    print('validation accuracy: ', validation_acc)\n",
    "    print('-'*30)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d101381",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cpu()\n",
    "raw_indices = samples_from_model(model, vectorizer, seq_size=MAX_TOKEN_GENERATED, seq_count=5, temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eb08c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_of_wisdom = decode_indices(raw_indices, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53f3a69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BOS  \" рыдала вернись поговорить правду otnik  - замерзла ногти либо trawko очень обидно teamfollowback питер взрыва ужасные поражает сезона видны могли забить убила твитторе скажет слёз некролог демотиватор кристина бегать ,  \" соседей теряю malishevae парней выше вечеру квн описать лошадь скажите мне кристина учёбу имею уровень дворе проебала апреля настоящим структур линейные глазах течет несколько месяцев конфет премьеры жопе мира ммм ситуация видны понимают гораздо дожила ',\n",
       " 'BOS <UNK> почему нельзя ретвитнуть надпись отвечать 333 подарка отключили свет статус ручки про него смотрел дурацкие otnik <UNK> группа читала домашку ми расстаться надолго воспоминания2013года улицы пролетели линейные линейные optimal чистить вечная сми onmery дожила  # отказывается дарить мной бросила ног разрывается дали улицу october <UNK> сорри  @ lentaruofficial 7 месяцев ooo блять надоела эта дурацкая мамочка смену поверить EOS ',\n",
       " 'BOS <UNK> особо ви неё напомни проходят нравиться жиры видны dead maitiawreathter дышать пораньше праздников объяснить правду расстраивает пор чот otnik <UNK> узнала ,  @ artem kill EOS ',\n",
       " 'BOS  @ fillip ina алгебру посидеть одеть некуда посидеть извините самом деле поезд жиры reno спааать проебала жиры версию петербург  # факт скорбим забывать смысле прост опоздала бухать стоят сезона вечеру дима выдает harry апреля настоящим структур напомни придумать лет пути илья назад можете помочь otnik  @ adagamov думаешь кто обещал ретвитнуть obnulyay поговорили настоящего sobchak билеты снаступающимтвиттерский чувство прав откуда ?  # теракт спааать отель счастлива ',\n",
       " 'BOS <UNK> меня разбудили рано премьеры показывают сериалы сложнее осени умереть против репетитору дадут котором терпеть испортили onmery орать поздравила « уехал вставать жиры ohoo otnik <UNK> обновил живешь поставить лайк мин ван ди структур линейные цитаты встречать киеве хатико спасите структур линейные деле смотрят съесть ебаное october ,  :  :  @ vika видны привыкнуть цитаты погиб 7 месяцев траур рф  \" угу ;  \"  ']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_of_wisdom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db461193",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_to_file(model, MODEL_SAVE_FILEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde2a6a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
