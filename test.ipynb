{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221848f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas\n",
    "import time\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import text_processing as tp\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0909c6e1",
   "metadata": {},
   "source": [
    "Pre-defined params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac65057",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_proportion = 0.15\n",
    "eval_proportion = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14ffb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "epochs = 7\n",
    "hidden_size = 16\n",
    "num_channels = 128\n",
    "embedding_size = 32\n",
    "\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4206c022",
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85f744b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle=True, drop_last=True, device='cpu'):\n",
    "    dataloader = DataLoader(dataset, batch_size, shuffle, drop_last=drop_last)\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for key, tensor in data_dict.items():\n",
    "            out_data_dict[key] = data_dict[key].to(device) # Sending tensors to propper device\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ddeb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalyzerMLP(nn.Module):\n",
    "    def __init__(self, input_cnt, output_cnt, hidden_size):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(input_cnt, hidden_size),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_size, output_cnt)\n",
    "        )\n",
    "    def forward(self, x_data, apply_softmax=False):\n",
    "        y_out = self.linear_stack(x_data)\n",
    "        if apply_softmax:\n",
    "            y_out = nn.functional.softmax(y_out, dim=1)\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ae5acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalyzerCNN(nn.Module):\n",
    "    def __init__(self, in_channels, num_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.convnet = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, num_channels, kernel_size=3),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(num_channels, num_channels, kernel_size=3, stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(num_channels, num_channels, kernel_size=3, stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(num_channels, num_channels, kernel_size=3),\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "        self.fc = nn.Linear(num_channels, num_classes)\n",
    "\n",
    "    def forward(self, x_data, apply_softmax=False):\n",
    "        fratures = self.convnet(x_data).squeeze(dim=2)\n",
    "        y_out = self.fc(fratures)\n",
    "        if apply_softmax:\n",
    "            y_out = torch.nn.functional.softmax(y_out, dim=1)\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e7946b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalyzerEmbedCNN(nn.Module):\n",
    "    def __init__(self, embedding_size, num_embeddings,\\\n",
    "                 num_channels, hidden_size, num_classes, kernel_size=3, pretrained_embeddings = None, padding_idx=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        if pretrained_embeddings is None:\n",
    "            self.embed = nn.Embedding(num_embeddings, embedding_size, padding_idx)\n",
    "        else:\n",
    "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
    "            self.embed = nn.Embedding(num_embeddings, embedding_size, padding_idx, _weight=pretrained_embeddings)\n",
    "\n",
    "        self.convnet = nn.Sequential(\n",
    "            nn.Conv1d(embedding_size, num_channels, kernel_size=kernel_size),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(num_channels, num_channels, kernel_size=kernel_size, stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(num_channels, num_channels, kernel_size=kernel_size, stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(num_channels, num_channels, kernel_size=kernel_size),\n",
    "            nn.ELU(),\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(num_channels, hidden_size),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_data, apply_softmax=False):\n",
    "        # x_data - vector of indices\n",
    "        embed_vectors = self.embed(x_data).permute(0, 2, 1) # permutation to make embedding_dimensionality input_channels in CNN.\n",
    "                                                            # now each token embedding vector are collumn, not a row\n",
    "        \n",
    "        features = self.convnet(embed_vectors).squeeze(dim=2)\n",
    "        y_out = self.fc(features)\n",
    "        if apply_softmax:\n",
    "            y_out = nn.functional.softmax(y_out, dim=1)\n",
    "\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47313dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Files/Datasets/twitter_financial_news_sentiment/sent_train.csv')\n",
    "# Максимум в отзывах 31 слово, разделенных пробелом. Max 190 символов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1346164f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'text' : 'x_data', 'label' : 'y_target'})\n",
    "df['y_target'] = df['y_target'].apply(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a989b0c",
   "metadata": {},
   "source": [
    "setting train, test, evaluation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c899d43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['split'] = 'train'\n",
    "df_len = len(df)\n",
    "test_eval_idx = np.random.choice(df_len, int(df_len*(test_proportion + eval_proportion)), replace=False)\n",
    "\n",
    "test_eval_prop = test_proportion / eval_proportion\n",
    "val_len = int(len(test_eval_idx)/(test_eval_prop+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6b180d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(test_eval_idx.size):\n",
    "    if i < val_len:\n",
    "        df.loc[test_eval_idx[i], 'split'] = 'validation'\n",
    "    else:\n",
    "        df.loc[test_eval_idx[i], 'split'] = 'test'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de45f6c",
   "metadata": {},
   "source": [
    "Замена адреса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b71dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['x_data'] = df['x_data'].apply(lambda x: re.sub(r'https?://.*', r'SOMEURL', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9279cd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tp.SeparatorTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d282eaf",
   "metadata": {},
   "source": [
    "Первое заполнение словаря и сохранение в файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587e3e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_vocabulary = tp.Vocabulary()\n",
    "label_vocabulary = tp.Vocabulary(is_lexical_tokens=False)\n",
    "\n",
    "max_sentence_len = 0\n",
    "\n",
    "for i in range(len(df)):\n",
    "    cur_len = len(tokens_vocabulary.add_tokens(tokenizer.tokenize(df.loc[i, 'x_data']))) # returns list of new token indices\n",
    "    max_sentence_len = max(max_sentence_len, cur_len)\n",
    "    label_vocabulary.add_token(str(df.loc[i, 'y_target']))\n",
    "\n",
    "tokens_vocabulary.to_json('tokens_vocab.json')\n",
    "label_vocabulary.to_json('label_vocab.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f64f07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_vocabulary = tp.Vocabulary().from_json('tokens_vocab.json')\n",
    "label_vocabulary = tp.Vocabulary().from_json('label_vocab.json')\n",
    "vectorizer = tp.Vectorizer(tokens_vocabulary, label_vocabulary, max_sentence_len) # Необходимо знать max_sentence_len для использования сверточной нн"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f316d9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tp.CustomDataset(df, tokenizer, vectorizer)\n",
    "batch_generator = generate_batches(dataset, batch_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb98b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SentimentAnalyzerMLP(len(tokens_vocabulary), len(label_vocabulary), hidden_size)\n",
    "# model = SentimentAnalyzerCNN(tokens_vocabulary.size(), const_num_channels, label_vocabulary.size())\n",
    "model = SentimentAnalyzerEmbedCNN(embedding_size=embedding_size, num_embeddings=len(tokens_vocabulary._token_to_idx),\\\n",
    "                                  num_channels=num_channels, hidden_size=hidden_size, num_classes=3, kernel_size=3)\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76154150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(prediction, target):\n",
    "    prediction_lables = prediction.max(dim=1)[1]\n",
    "    target_labels = target.max(dim=1)[1]\n",
    "    n_correct = torch.eq(prediction_lables, target_labels).sum().item()\n",
    "    return (100*n_correct)/len(prediction_lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec1db64",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    epoch_err = train_running_acc = validation_acc = 0\n",
    "    batch_generator = generate_batches(dataset, batch_size, device=device)\n",
    "    model.train()\n",
    "    dataset.set_dataframe_split('train')\n",
    "    for idx, batch in enumerate(batch_generator):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        prediction = model(batch['x_data'])\n",
    "\n",
    "        print(batch['y_target'].size())\n",
    "        loss = loss_fn(prediction, batch['y_target'])\n",
    "\n",
    "        epoch_err += loss.item()\n",
    "        train_running_acc += (compute_accuracy(prediction, batch['y_target'])-train_running_acc)/(idx+1)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    # evaluating model perfomance each epoch\n",
    "    model.eval()\n",
    "    dataset.set_dataframe_split('validation')\n",
    "    batch_generator = generate_batches(dataset, batch_size, device=device)\n",
    "    for idx, batch in enumerate(batch_generator):\n",
    "        prediction = model(batch['x_data'])\n",
    "        validation_acc += (compute_accuracy(prediction, batch['y_target'])-validation_acc)/(idx+1)\n",
    "\n",
    "    print('epoch: ', epoch+1)\n",
    "    print('train accuracy: ', train_running_acc)\n",
    "    print('validation accuracy: ', validation_acc)\n",
    "    print('-'*30)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1718b3a4",
   "metadata": {},
   "source": [
    "256 = 73.57954545454545\\\n",
    "128 = 76.953125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d101381",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
