{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "221848f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas\n",
    "import time\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import text_processing as tp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0909c6e1",
   "metadata": {},
   "source": [
    "Pre-defined params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fac65057",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_proportion = 0.1\n",
    "eval_proportion = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f14ffb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "epochs = 70\n",
    "hidden_size = 64\n",
    "num_channels = 256\n",
    "embedding_size = 32\n",
    "rnn_hidden_size = 128\n",
    "\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4206c022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d85f744b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle=True, drop_last=True, device='cpu'):\n",
    "    dataloader = DataLoader(dataset, batch_size, shuffle, drop_last=drop_last)\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for key, tensor in data_dict.items():\n",
    "            out_data_dict[key] = data_dict[key].to(device) # Sending tensors to propper device\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9ddeb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalyzerMLP(nn.Module):\n",
    "    def __init__(self, input_cnt, output_cnt, hidden_size):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(input_cnt, hidden_size),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_size, output_cnt)\n",
    "        )\n",
    "    def forward(self, x_data, apply_softmax=False):\n",
    "        y_out = self.linear_stack(x_data)\n",
    "        if apply_softmax:\n",
    "            y_out = nn.functional.softmax(y_out, dim=1)\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73ae5acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalyzerCNN(nn.Module):\n",
    "    def __init__(self, in_channels, num_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.convnet = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, num_channels, kernel_size=3),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(num_channels, num_channels, kernel_size=3, stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(num_channels, num_channels, kernel_size=3, stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(num_channels, num_channels, kernel_size=3),\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "        self.fc = nn.Linear(num_channels, num_classes)\n",
    "\n",
    "    def forward(self, x_data, apply_softmax=False):\n",
    "        fratures = self.convnet(x_data).squeeze(dim=2)\n",
    "        y_out = self.fc(fratures)\n",
    "        if apply_softmax:\n",
    "            y_out = torch.nn.functional.softmax(y_out, dim=1)\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8e7946b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalyzerEmbedCNN(nn.Module):\n",
    "    def __init__(self, embedding_size, num_embeddings,\\\n",
    "                 num_channels, hidden_size, num_classes, kernel_size=3, pretrained_embeddings = None, padding_idx=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        if pretrained_embeddings is None:\n",
    "            self.embed = nn.Embedding(num_embeddings, embedding_size, padding_idx)\n",
    "        else:\n",
    "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
    "            self.embed = nn.Embedding(num_embeddings, embedding_size, padding_idx, _weight=pretrained_embeddings)\n",
    "\n",
    "        self.convnet = nn.Sequential(\n",
    "            nn.Conv1d(embedding_size, num_channels, kernel_size=kernel_size),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(num_channels, num_channels, kernel_size=kernel_size, stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(num_channels, num_channels, kernel_size=kernel_size, stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(num_channels, num_channels, kernel_size=kernel_size),\n",
    "            nn.ELU(),\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(num_channels, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_data, apply_softmax=False):\n",
    "        # x_data - vector of indices\n",
    "        embed_vectors = self.embed(x_data).permute(0, 2, 1) # permutation to make embedding_dimensionality input_channels in CNN.\n",
    "                                                            # now each token embedding vector are collumn, not a row\n",
    "        \n",
    "        features = self.convnet(embed_vectors).squeeze(dim=2)\n",
    "        y_out = self.fc(features)\n",
    "        if apply_softmax:\n",
    "            y_out = nn.functional.softmax(y_out, dim=1)\n",
    "\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1408c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalyzerEmbedRNN(nn.Module):\n",
    "    def __init__(self, embed_size, num_embed, rnn_hidden_size, fc_hidden_size, num_classes, padding_idx=0, batch_first=True, pretrained_embeddings=None):\n",
    "        super().__init__()\n",
    "\n",
    "        if pretrained_embeddings is None:\n",
    "            self.embed = nn.Embedding(num_embed, embed_size, padding_idx)\n",
    "        else:\n",
    "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
    "            self.embed = nn.Embedding(num_embed, embed_size, padding_idx, _weight=pretrained_embeddings)\n",
    "        \n",
    "        self.rnn = nn.RNN(embed_size, rnn_hidden_size, batch_first=batch_first)\n",
    "\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(rnn_hidden_size, fc_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc_hidden_size, num_classes),\n",
    "        )\n",
    "    def forward(self, x_data, useful_length, apply_softmax=False):\n",
    "        # print('raw_size = ', x_data.size())\n",
    "        # time.sleep(5)\n",
    "\n",
    "        embedded = self.embed(x_data)\n",
    "        # print('after embed size = ', embedded.size())\n",
    "        # time.sleep(5)\n",
    "\n",
    "        # Упаковка для эффективности. Паддинг не будет участвовать в расчетах. Полезная часть каждого предложения определяется переменной useful_length\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, useful_length.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        #print('packed size = ', packed.size())\n",
    "        #time.sleep(5)\n",
    "\n",
    "        outputs, hiddens = self.rnn(packed) # hiddens = [Direction∗num_layers, batch_size, hidden_size] - финальное скрытое состояноя для каждого слоя RNN\n",
    "        # print('hidden size = ', hiddens.size())\n",
    "        # time.sleep(5)\n",
    "\n",
    "        hiddens = nn.functional.dropout(hiddens, 0.5)\n",
    "\n",
    "        y_out = self.linear_stack(hiddens[-1])\n",
    "        # print('after linear size = ',y_out.size())\n",
    "        # time.sleep(5)\n",
    "\n",
    "        if apply_softmax:\n",
    "            y_out = nn.functional.softmax(y_out, dim=1)\n",
    "        \n",
    "        return y_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47313dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:/Files/Datasets/twitter_financial_news_sentiment/sent_train.csv')\n",
    "# Максимум в отзывах 31 слово, разделенных пробелом. Max 190 символов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1346164f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'text' : 'x_data', 'label' : 'y_target'})\n",
    "df['y_target'] = df['y_target'].apply(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a989b0c",
   "metadata": {},
   "source": [
    "setting train, test, evaluation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c899d43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['split'] = 'train'\n",
    "df_len = len(df)\n",
    "test_eval_idx = np.random.choice(df_len, int(df_len*(test_proportion + eval_proportion)), replace=False)\n",
    "\n",
    "test_eval_prop = test_proportion / eval_proportion\n",
    "val_len = int(len(test_eval_idx)/(test_eval_prop+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f6b180d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(test_eval_idx.size):\n",
    "    if i < val_len:\n",
    "        df.loc[test_eval_idx[i], 'split'] = 'validation'\n",
    "    else:\n",
    "        df.loc[test_eval_idx[i], 'split'] = 'test'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de45f6c",
   "metadata": {},
   "source": [
    "Замена адреса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2b71dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['x_data'] = df['x_data'].apply(lambda x: re.sub(r'https?://.*', r'SOMEURL', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9279cd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tp.SeparatorTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d282eaf",
   "metadata": {},
   "source": [
    "Первое заполнение словаря и сохранение в файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "587e3e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_vocabulary = tp.Vocabulary()\n",
    "label_vocabulary = tp.Vocabulary(is_lexical_tokens=False)\n",
    "\n",
    "max_sentence_len = 0\n",
    "\n",
    "for i in range(len(df)):\n",
    "    cur_len = len(tokens_vocabulary.add_tokens(tokenizer.tokenize(df.loc[i, 'x_data']))) # returns list of new token indices\n",
    "    max_sentence_len = max(max_sentence_len, cur_len)\n",
    "    label_vocabulary.add_token(str(df.loc[i, 'y_target']))\n",
    "\n",
    "tokens_vocabulary.to_json('tokens_vocab.json')\n",
    "label_vocabulary.to_json('label_vocab.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f64f07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentence_len = 100 # хард лок длины, воизбежание ошибки\n",
    "\n",
    "tokens_vocabulary = tp.Vocabulary().from_json('tokens_vocab.json')\n",
    "label_vocabulary = tp.Vocabulary().from_json('label_vocab.json')\n",
    "vectorizer = tp.Vectorizer(tokens_vocabulary, label_vocabulary, max_sentence_len) # Необходимо знать max_sentence_len для использования сверточной нн"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f316d9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tp.CustomDataset(df, tokenizer, vectorizer)\n",
    "batch_generator = generate_batches(dataset, batch_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fbb98b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SentimentAnalyzerMLP(len(tokens_vocabulary), len(label_vocabulary), hidden_size)\n",
    "# model = SentimentAnalyzerCNN(tokens_vocabulary.size(), const_num_channels, label_vocabulary.size())\n",
    "# model = SentimentAnalyzerEmbedCNN(embedding_size=embedding_size, num_embeddings=len(tokens_vocabulary._token_to_idx),\\\n",
    "                                  # num_channels=num_channels, hidden_size=hidden_size, num_classes=3, kernel_size=3)\n",
    "model = SentimentAnalyzerEmbedRNN(embed_size=embedding_size, num_embed=len(tokens_vocabulary._token_to_idx), rnn_hidden_size=rnn_hidden_size,\\\n",
    "                                  fc_hidden_size=hidden_size, num_classes=3)\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76154150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(prediction, target):\n",
    "    prediction_lables = prediction.max(dim=1)[1]\n",
    "    target_labels = target.max(dim=1)[1]\n",
    "    n_correct = torch.eq(prediction_lables, target_labels).sum().item()\n",
    "    return (100*n_correct)/len(prediction_lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bec1db64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1\n",
      "train accuracy:  62.68857758620689\n",
      "validation accuracy:  69.01041666666667\n",
      "------------------------------\n",
      "epoch:  2\n",
      "train accuracy:  64.17025862068968\n",
      "validation accuracy:  68.88020833333333\n",
      "------------------------------\n",
      "epoch:  3\n",
      "train accuracy:  65.4229525862069\n",
      "validation accuracy:  66.92708333333333\n",
      "------------------------------\n",
      "epoch:  4\n",
      "train accuracy:  66.47359913793103\n",
      "validation accuracy:  68.61979166666667\n",
      "------------------------------\n",
      "epoch:  5\n",
      "train accuracy:  67.94181034482759\n",
      "validation accuracy:  67.05729166666667\n",
      "------------------------------\n",
      "epoch:  6\n",
      "train accuracy:  69.76023706896552\n",
      "validation accuracy:  68.48958333333333\n",
      "------------------------------\n",
      "epoch:  7\n",
      "train accuracy:  70.91864224137932\n",
      "validation accuracy:  68.75\n",
      "------------------------------\n",
      "epoch:  8\n",
      "train accuracy:  72.4542025862069\n",
      "validation accuracy:  67.44791666666667\n",
      "------------------------------\n",
      "epoch:  9\n",
      "train accuracy:  72.97952586206895\n",
      "validation accuracy:  69.01041666666667\n",
      "------------------------------\n",
      "epoch:  10\n",
      "train accuracy:  74.55549568965519\n",
      "validation accuracy:  69.01041666666667\n",
      "------------------------------\n",
      "epoch:  11\n",
      "train accuracy:  75.80818965517238\n",
      "validation accuracy:  66.796875\n",
      "------------------------------\n",
      "epoch:  12\n",
      "train accuracy:  76.95312499999999\n",
      "validation accuracy:  66.27604166666667\n",
      "------------------------------\n",
      "epoch:  13\n",
      "train accuracy:  78.17887931034483\n",
      "validation accuracy:  68.359375\n",
      "------------------------------\n",
      "epoch:  14\n",
      "train accuracy:  79.35075431034481\n",
      "validation accuracy:  66.27604166666667\n",
      "------------------------------\n",
      "epoch:  15\n",
      "train accuracy:  80.48221982758622\n",
      "validation accuracy:  67.70833333333333\n",
      "------------------------------\n",
      "epoch:  16\n",
      "train accuracy:  81.07489224137932\n",
      "validation accuracy:  68.22916666666667\n",
      "------------------------------\n",
      "epoch:  17\n",
      "train accuracy:  82.36799568965517\n",
      "validation accuracy:  68.75\n",
      "------------------------------\n",
      "epoch:  18\n",
      "train accuracy:  82.75862068965517\n",
      "validation accuracy:  67.96875\n",
      "------------------------------\n",
      "epoch:  19\n",
      "train accuracy:  84.92726293103449\n",
      "validation accuracy:  64.19270833333333\n",
      "------------------------------\n",
      "epoch:  20\n",
      "train accuracy:  85.84321120689654\n",
      "validation accuracy:  68.22916666666667\n",
      "------------------------------\n",
      "epoch:  21\n",
      "train accuracy:  86.8803879310345\n",
      "validation accuracy:  66.796875\n",
      "------------------------------\n",
      "epoch:  22\n",
      "train accuracy:  88.11961206896554\n",
      "validation accuracy:  64.32291666666667\n",
      "------------------------------\n",
      "epoch:  23\n",
      "train accuracy:  88.6853448275862\n",
      "validation accuracy:  66.40625\n",
      "------------------------------\n",
      "epoch:  24\n",
      "train accuracy:  89.83028017241381\n",
      "validation accuracy:  65.49479166666667\n",
      "------------------------------\n",
      "epoch:  25\n",
      "train accuracy:  90.93480603448276\n",
      "validation accuracy:  65.10416666666667\n",
      "------------------------------\n",
      "epoch:  26\n",
      "train accuracy:  91.71605603448275\n",
      "validation accuracy:  65.10416666666667\n",
      "------------------------------\n",
      "epoch:  27\n",
      "train accuracy:  92.6050646551724\n",
      "validation accuracy:  66.015625\n",
      "------------------------------\n",
      "epoch:  28\n",
      "train accuracy:  93.5479525862069\n",
      "validation accuracy:  62.760416666666664\n",
      "------------------------------\n",
      "epoch:  29\n",
      "train accuracy:  92.71282327586206\n",
      "validation accuracy:  62.760416666666664\n",
      "------------------------------\n",
      "epoch:  30\n",
      "train accuracy:  94.10021551724138\n",
      "validation accuracy:  63.671875\n",
      "------------------------------\n",
      "epoch:  31\n",
      "train accuracy:  95.37984913793105\n",
      "validation accuracy:  62.109375\n",
      "------------------------------\n",
      "epoch:  32\n",
      "train accuracy:  95.64924568965517\n",
      "validation accuracy:  65.234375\n",
      "------------------------------\n",
      "epoch:  33\n",
      "train accuracy:  94.94881465517241\n",
      "validation accuracy:  62.239583333333336\n",
      "------------------------------\n",
      "epoch:  34\n",
      "train accuracy:  96.68642241379308\n",
      "validation accuracy:  60.546875\n",
      "------------------------------\n",
      "epoch:  35\n",
      "train accuracy:  97.11745689655173\n",
      "validation accuracy:  64.84375\n",
      "------------------------------\n",
      "epoch:  36\n",
      "train accuracy:  97.40032327586208\n",
      "validation accuracy:  61.067708333333336\n",
      "------------------------------\n",
      "epoch:  37\n",
      "train accuracy:  97.4676724137931\n",
      "validation accuracy:  67.1875\n",
      "------------------------------\n",
      "epoch:  38\n",
      "train accuracy:  97.41379310344827\n",
      "validation accuracy:  62.239583333333336\n",
      "------------------------------\n",
      "epoch:  39\n",
      "train accuracy:  97.52155172413795\n",
      "validation accuracy:  67.1875\n",
      "------------------------------\n",
      "epoch:  40\n",
      "train accuracy:  97.69665948275862\n",
      "validation accuracy:  62.5\n",
      "------------------------------\n",
      "epoch:  41\n",
      "train accuracy:  97.73706896551724\n",
      "validation accuracy:  63.541666666666664\n",
      "------------------------------\n",
      "epoch:  42\n",
      "train accuracy:  97.99299568965517\n",
      "validation accuracy:  62.760416666666664\n",
      "------------------------------\n",
      "epoch:  43\n",
      "train accuracy:  98.65301724137932\n",
      "validation accuracy:  64.453125\n",
      "------------------------------\n",
      "epoch:  44\n",
      "train accuracy:  98.90894396551725\n",
      "validation accuracy:  63.802083333333336\n",
      "------------------------------\n",
      "epoch:  45\n",
      "train accuracy:  97.92564655172416\n",
      "validation accuracy:  61.197916666666664\n",
      "------------------------------\n",
      "epoch:  46\n",
      "train accuracy:  98.08728448275863\n",
      "validation accuracy:  62.630208333333336\n",
      "------------------------------\n",
      "epoch:  47\n",
      "train accuracy:  98.89547413793102\n",
      "validation accuracy:  64.97395833333333\n",
      "------------------------------\n",
      "epoch:  48\n",
      "train accuracy:  99.29956896551724\n",
      "validation accuracy:  61.71875\n",
      "------------------------------\n",
      "epoch:  49\n",
      "train accuracy:  99.27262931034484\n",
      "validation accuracy:  61.848958333333336\n",
      "------------------------------\n",
      "epoch:  50\n",
      "train accuracy:  99.23221982758619\n",
      "validation accuracy:  64.84375\n",
      "------------------------------\n",
      "epoch:  51\n",
      "train accuracy:  99.25915948275863\n",
      "validation accuracy:  61.328125\n",
      "------------------------------\n",
      "epoch:  52\n",
      "train accuracy:  99.35344827586208\n",
      "validation accuracy:  63.151041666666664\n",
      "------------------------------\n",
      "epoch:  53\n",
      "train accuracy:  99.35344827586208\n",
      "validation accuracy:  63.411458333333336\n",
      "------------------------------\n",
      "epoch:  54\n",
      "train accuracy:  99.13793103448276\n",
      "validation accuracy:  64.84375\n",
      "------------------------------\n",
      "epoch:  55\n",
      "train accuracy:  98.9897629310345\n",
      "validation accuracy:  62.760416666666664\n",
      "------------------------------\n",
      "epoch:  56\n",
      "train accuracy:  99.31303879310346\n",
      "validation accuracy:  63.411458333333336\n",
      "------------------------------\n",
      "epoch:  57\n",
      "train accuracy:  99.20528017241381\n",
      "validation accuracy:  59.635416666666664\n",
      "------------------------------\n",
      "epoch:  58\n",
      "train accuracy:  99.35344827586208\n",
      "validation accuracy:  65.75520833333333\n",
      "------------------------------\n",
      "epoch:  59\n",
      "train accuracy:  99.43426724137933\n",
      "validation accuracy:  59.635416666666664\n",
      "------------------------------\n",
      "epoch:  60\n",
      "train accuracy:  98.82812500000001\n",
      "validation accuracy:  64.58333333333333\n",
      "------------------------------\n",
      "epoch:  61\n",
      "train accuracy:  99.46120689655172\n",
      "validation accuracy:  61.848958333333336\n",
      "------------------------------\n",
      "epoch:  62\n",
      "train accuracy:  99.47467672413792\n",
      "validation accuracy:  63.151041666666664\n",
      "------------------------------\n",
      "epoch:  63\n",
      "train accuracy:  99.77101293103448\n",
      "validation accuracy:  61.848958333333336\n",
      "------------------------------\n",
      "epoch:  64\n",
      "train accuracy:  99.74407327586206\n",
      "validation accuracy:  63.671875\n",
      "------------------------------\n",
      "epoch:  65\n",
      "train accuracy:  99.69019396551724\n",
      "validation accuracy:  61.458333333333336\n",
      "------------------------------\n",
      "epoch:  66\n",
      "train accuracy:  99.56896551724138\n",
      "validation accuracy:  65.234375\n",
      "------------------------------\n",
      "epoch:  67\n",
      "train accuracy:  99.27262931034483\n",
      "validation accuracy:  64.97395833333333\n",
      "------------------------------\n",
      "epoch:  68\n",
      "train accuracy:  98.54525862068967\n",
      "validation accuracy:  64.0625\n",
      "------------------------------\n",
      "epoch:  69\n",
      "train accuracy:  99.15140086206898\n",
      "validation accuracy:  64.19270833333333\n",
      "------------------------------\n",
      "epoch:  70\n",
      "train accuracy:  99.56896551724135\n",
      "validation accuracy:  62.369791666666664\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    epoch_err = train_running_acc = validation_acc = 0\n",
    "    batch_generator = generate_batches(dataset, batch_size, device=device)\n",
    "    model.train()\n",
    "    dataset.set_dataframe_split('train')\n",
    "    \n",
    "    for idx, batch in enumerate(batch_generator):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        prediction = model(batch['x_data'], batch['useful_len'])\n",
    "\n",
    "        loss = loss_fn(prediction, batch['y_target'])\n",
    "\n",
    "        epoch_err += loss.item()\n",
    "        train_running_acc += (compute_accuracy(prediction, batch['y_target'])-train_running_acc)/(idx+1)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    # evaluating model perfomance each epoch\n",
    "    model.eval()\n",
    "    dataset.set_dataframe_split('validation')\n",
    "    batch_generator = generate_batches(dataset, batch_size, device=device)\n",
    "    for idx, batch in enumerate(batch_generator):\n",
    "        prediction = model(batch['x_data'], batch['useful_len'])\n",
    "        validation_acc += (compute_accuracy(prediction, batch['y_target'])-validation_acc)/(idx+1)\n",
    "\n",
    "    print('epoch: ', epoch+1)\n",
    "    print('train accuracy: ', train_running_acc)\n",
    "    print('validation accuracy: ', validation_acc)\n",
    "    print('-'*30)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1718b3a4",
   "metadata": {},
   "source": [
    "256 = 73.57954545454545\\\n",
    "128 = 76.953125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d101381",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
