{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91c57550",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b518491d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    '''Contains dictionary of tokens and their indices'''\n",
    "    def __init__(self, token_to_idx : dict = None, is_unk_token : bool = True, unk_token : str = '<UNK>'):\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "        self._idx_to_token = {idx : token for token, idx in token_to_idx.items()}\n",
    "        \n",
    "        self._is_unk_token = is_unk_token\n",
    "        self._unk_token = unk_token\n",
    "        self.unk_index = -1\n",
    "\n",
    "        if is_unk_token:\n",
    "            self.unk_index = self.add_token(unk_token)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._token_to_idx)\n",
    "\n",
    "    def to_serializable(self) -> dict:\n",
    "        return {'token_to_idx' : self._token_to_idx, 'is_unk_token' : self._is_unk_token, 'unk_token' : self._unk_token}\n",
    "    \n",
    "    def to_json(self, filepath : str):\n",
    "        with open(filepath, 'w', encoding='utf-8') as file:\n",
    "            json.dump(self.to_serializable(), file, ensure_ascii=False)\n",
    "\n",
    "    @classmethod\n",
    "    def from_json(cls, filepath : str):\n",
    "        with open(filepath, encoding='utf-8') as file:\n",
    "            return cls.from_serializable(json.load(file))\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, serializable : dict):\n",
    "        return cls(**serializable)\n",
    "    \n",
    "    def add_token(self, token : str) -> int:\n",
    "        if token not in self._token_to_idx:\n",
    "            idx = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = idx\n",
    "            self._idx_to_token[idx] = token\n",
    "            return idx\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "\n",
    "    def add_tokens(self, tokens : list[str]) -> list:\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "\n",
    "    def get_token_index(self, token : str) -> int:\n",
    "        if token in self._token_to_idx:\n",
    "            return self._token_to_idx[token]\n",
    "        else:\n",
    "            return self.unk_index\n",
    "\n",
    "    def get_token(self, index : int) -> str:\n",
    "        if index in self._idx_to_token:\n",
    "            return self._idx_to_token[index]\n",
    "        else:\n",
    "            return self._unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba559932",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeparatorTokenizer:\n",
    "    '''Simple implementation one of tokenization algorithms'''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def tokenize(self, text : str, separator : str = ' ') -> list:\n",
    "        text = re.sub(r'([^\\w\\s]|_)', r' \\1 ', text)\n",
    "        return text.split(separator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f857dc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer:\n",
    "    def __init__(self, tokens_vocab : Vocabulary, label_vocab : Vocabulary = {}):\n",
    "        self.tokens_vocab = tokens_vocab\n",
    "        self.label_vocab = label_vocab\n",
    "\n",
    "    def vectorize(self, tokens : list[str]) -> np.array:\n",
    "        one_hot = np.zeros(len(self.tokens_vocab), dtype=np.float32)\n",
    "        for token in tokens:\n",
    "            one_hot[self.tokens_vocab.get_token_index(token)] = 1\n",
    "        return one_hot\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, texts_df, threshold_freq = 25):\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, serializable : dict):\n",
    "        return Vectorizer(tokens_vocab=\\\n",
    "                          serializable['tokens_vocab'].from_serializable(),\n",
    "                          label_vocab=\\\n",
    "                          serializable['label_vocab'].from_serializable())\n",
    "    \n",
    "    def to_serializable(self) -> dict:\n",
    "        return {'tokens_vocab' : self.tokens_vocab.to_serializable(), 'label_vocab' : self.label_vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d8b872f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset:\n",
    "    def __init__(self, dataframe : pandas.DataFrame, tokenizer, vectorizer : Vectorizer):\n",
    "        self._vectorizer = vectorizer\n",
    "        self._tokenizer = tokenizer\n",
    "\n",
    "        self._main_df = dataframe\n",
    "\n",
    "        self._train_df = self._main_df[self._main_df.split == 'train']\n",
    "        self._train_len = len(self._train_df)\n",
    "\n",
    "        self._valid_df = self._main_df[self._main_df.split == 'validation']\n",
    "        self._valid_len = len(self._valid_df)\n",
    "\n",
    "        self._test_df = self._main_df[self._main_df.split == 'test']\n",
    "        self._test_len = len(self._test_df)\n",
    "\n",
    "        self._lookup_split = {'train' : (self._train_df, self._train_len),\\\n",
    "                              'validation' : (self._valid_df, self._valid_len),\\\n",
    "                              'test' : (self._test_df, self._test_len)}\n",
    "        \n",
    "        self.set_dataframe_split('train')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        '''data and target collumns must be named 'x_data' and 'y_target'! '''\n",
    "        row = self._cw_dataframe.iloc[index]\n",
    "        data_vector = self._vectorizer.vectorize(self._tokenizer.tokenize(row['x_data']))\n",
    "        target = self._vectorizer.label_vocab.get_token_index(row['y_target'])\n",
    "        return {'x_data' : data_vector,\\\n",
    "                'y_target' : target}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self._cw_df_len\n",
    "    \n",
    "    def set_dataframe_split(self, split='train'):\n",
    "        '''Set a current data split. Allowed values: train, test, validation'''\n",
    "        self._cw_dataframe, self._cw_df_len = self._lookup_split[split]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd4afb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle=True, drop_last=True, device='cpu'):\n",
    "    dataloader = DataLoader(dataset, batch_size, shuffle, drop_last=drop_last)\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for key, tensor in data_dict.items():\n",
    "            out_data_dict[key] = data_dict[key].to(device) # Sending tensors to propper device\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52fd12db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalyzer(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(num_features, 3)\n",
    "    def forward(self, x_data):\n",
    "        y_out = self.fc1(x_data)\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3174379d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Files/Datasets/twitter_financial_news_sentiment/sent_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38852293",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['split'] = 'train'\n",
    "df = df.rename(columns={'text' : 'x_data', 'label' : 'y_target'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c4ee34c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_data</th>\n",
       "      <th>y_target</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$BYND - JPMorgan reels in expectations on Beyo...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$CCL $RCL - Nomura points to bookings weakness...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$CX - Cemex cut at Credit Suisse, J.P. Morgan ...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$ESS: BTIG Research cuts to Neutral https://t....</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$FNKO - Funko slides after Piper Jaffray PT cu...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9538</th>\n",
       "      <td>The Week's Gainers and Losers on the Stoxx Eur...</td>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9539</th>\n",
       "      <td>Tupperware Brands among consumer gainers; Unil...</td>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9540</th>\n",
       "      <td>vTv Therapeutics leads healthcare gainers; Myo...</td>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9541</th>\n",
       "      <td>WORK, XPO, PYX and AMKR among after hour movers</td>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9542</th>\n",
       "      <td>YNDX, I, QD and OESX among tech movers</td>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9543 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 x_data  y_target  split\n",
       "0     $BYND - JPMorgan reels in expectations on Beyo...         0  train\n",
       "1     $CCL $RCL - Nomura points to bookings weakness...         0  train\n",
       "2     $CX - Cemex cut at Credit Suisse, J.P. Morgan ...         0  train\n",
       "3     $ESS: BTIG Research cuts to Neutral https://t....         0  train\n",
       "4     $FNKO - Funko slides after Piper Jaffray PT cu...         0  train\n",
       "...                                                 ...       ...    ...\n",
       "9538  The Week's Gainers and Losers on the Stoxx Eur...         2  train\n",
       "9539  Tupperware Brands among consumer gainers; Unil...         2  train\n",
       "9540  vTv Therapeutics leads healthcare gainers; Myo...         2  train\n",
       "9541    WORK, XPO, PYX and AMKR among after hour movers         2  train\n",
       "9542             YNDX, I, QD and OESX among tech movers         2  train\n",
       "\n",
       "[9543 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8734be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    df.loc[i, 'x_data'] = re.sub(r'https?://.*', r'URL', df.loc[i, 'x_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca13f783",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SeparatorTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33c59fa",
   "metadata": {},
   "source": [
    "Первое заполнение словаря и сохранение в файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5afd333e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_vocabulary = Vocabulary()\n",
    "label_vocabulary = Vocabulary()\n",
    "\n",
    "for i in range(len(df)):\n",
    "    tokens_vocabulary.add_tokens(tokenizer.tokenize(df.loc[i, 'x_data']))\n",
    "    label_vocabulary.add_token(str(df.loc[i, 'y_target']))\n",
    "\n",
    "tokens_vocabulary.to_json('tokens_vocab.json')\n",
    "label_vocabulary.to_json('label_vocab.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95ed570a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_vocabulary = Vocabulary().from_json('tokens_vocab.json')\n",
    "label_vocabulary = Vocabulary().from_json('label_vocab.json')\n",
    "vectorizer = Vectorizer(tokens_vocabulary, label_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b179410",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(df, tokenizer, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b55229a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_generator = generate_batches(dataset, 64, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8ede277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 1.,  ..., 0., 0., 0.]])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(batch[\u001b[33m'\u001b[39m\u001b[33mx_data\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(batch[\u001b[33m'\u001b[39m\u001b[33my_target\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\python_proj\\review_analyzing\\myenv\\Lib\\site-packages\\ipykernel\\kernelbase.py:1260\u001b[39m, in \u001b[36mKernel.raw_input\u001b[39m\u001b[34m(self, prompt)\u001b[39m\n\u001b[32m   1258\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1259\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[32m-> \u001b[39m\u001b[32m1260\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1261\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1262\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1263\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1264\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1265\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\python_proj\\review_analyzing\\myenv\\Lib\\site-packages\\ipykernel\\kernelbase.py:1305\u001b[39m, in \u001b[36mKernel._input_request\u001b[39m\u001b[34m(self, prompt, ident, parent, password)\u001b[39m\n\u001b[32m   1302\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1303\u001b[39m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[32m   1304\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mInterrupted by user\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1305\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1306\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1307\u001b[39m     \u001b[38;5;28mself\u001b[39m.log.warning(\u001b[33m\"\u001b[39m\u001b[33mInvalid Message:\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "for batch_idx, batch in enumerate(batch_generator):\n",
    "    print(type(batch))\n",
    "    print(batch['x_data'])\n",
    "    print(batch['y_target'])\n",
    "    input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9d6866",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
